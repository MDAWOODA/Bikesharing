{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Speech Emotion Recognition.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOR/aI0A3M75Z02qZM3XeUt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MDAWOODA/Bikesharing/blob/main/Copy_of_Speech_Emotion_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np #linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import os # to use operating system dependent functionality\n",
        "import librosa # to extract speech features\n",
        "import wave  # read and write WAV files\n",
        "import matplotlib.pyplot as plt  # to generate the visualizations\n",
        "\n",
        "# NLP classifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# LSTM Classifier\n",
        "import keras\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.optimizer_v1 import rmsprop  "
      ],
      "metadata": {
        "id": "f-Pq47qm3HDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alW-LpLk05IT",
        "outputId": "fa4a9b15-643d-46bc-e9f4-6b9185189f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_mfcc(wav_file_name):\n",
        "  #this function extracts mfcc features and obtain mean of each dimension\n",
        "  #Input  : path_to_wav_file\n",
        "  #Output : mfcc_features'''\n",
        "  y, sr = librosa.load(wav_file_name)\n",
        "  mfcc  = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)\n",
        "\n",
        "  return mfcc "
      ],
      "metadata": {
        "id": "2IL1g3Pk51rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### load radvess speech data ###\n",
        "radvess_speech_labels = [] # to save exracted label/file\n",
        "radvess_speech_data   = [] # to save extracted features/file\n",
        "for dirname,_, filenames in os.walk('')\n",
        "   for filename in filenames :\n",
        "     #print (os.path.join(dirname, filename))\n",
        "     radvess_speech_labels.append(int(filename[7:8]) - 1) # the index 7 and 8 of the file name represent the emotion label\n",
        "     wav_file_name = os.path.join(dirname, filename)\n",
        "     radvess_speech_data.append(extart_mfcc(wav_file_name)) # extract mfcc features/file\n",
        "\n",
        "   print('finish loading the dataset')  \n"
      ],
      "metadata": {
        "id": "PZZkKvKG6_d8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ravdess_speech_data"
      ],
      "metadata": {
        "id": "V1vgzr7RjeB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### convert data and label to array\n",
        "ravdess_spech_data_array   = np.asarray(radvess_speech_data) # convert the input to array\n",
        "ravdess_speech_label_array = np.asarray(radvess_speech_label)\n",
        "ravdess_speech_label_array.shape #get tuple of array dimension\n",
        "\n",
        "### make categorical labels\n",
        "labels_categorical = to_categorical(ravdess_speech_label_array) # converts a class vector(integers) to binary class metrix\n",
        "labels_categorical.shape"
      ],
      "metadata": {
        "id": "aJa3pbSwjyR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ravdess_speech_data_array"
      ],
      "metadata": {
        "id": "PDeejtn8lRMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test=train_test_split(np.array(ravdess_speech_data_array),labels_categorical, test_size=0.20, random_state=9 )"
      ],
      "metadata": {
        "id": "KMWhQBmflXU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split the training, validating, and testing sets\n",
        "number_of_samples = ravdess_speech_data_array.shape[0]\n",
        "training_samples = int(number_of_samples*0.8)\n",
        "validation_samples = int(number_of_samples*0.1)\n",
        "test_samples = int(number_of_samples*0.1)"
      ],
      "metadata": {
        "id": "ba_hHnTPl8JN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the LSTM model\n",
        "def create_model_LSTM():\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(128, return_sequences=False, input_shape=(40,1)))\n",
        "  model.add(Dense(64))\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(32))\n",
        "  model.add(Dropout(0.4))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(Dense(8))\n",
        "  model.add(Activation('softmax'))\n",
        "\n",
        "  # configures the model for training\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "RB4-vM50nHYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.expand_dims(ravdess_speech_data_array[:training_samples],-1)"
      ],
      "metadata": {
        "id": "z0v8pCNPor97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w.shape"
      ],
      "metadata": {
        "id": "V9ZCbe26osM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_A =create_model_LSTM()\n",
        "history = model_A.fit(np.expand_dims(ravdess_speech_data_array[:training_samples],-1),labels_categorical[:training_sample],validation_data=(np.expand_dims(ravdess_speech_data_array[training_samples:training_samples + validation_samples],-1), labels_categorical[training_samples:training_samples + validation_samples], epochs=130, shuffle=True) )"
      ],
      "metadata": {
        "id": "WfVcCJH9osZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### loss plots using LSTM model\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1,len(loss)+  1)\n",
        "\n",
        "plt.plot(epochs, loss, 'ro', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.tittle('Training and Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q_30tx3oosks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### accuracy plots using LSTM model\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accurcy']\n",
        "\n",
        "plt.plot(epochs, acc, 'ro', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b',label ='Validation acc')\n",
        "plt.tittle('Training and Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show"
      ],
      "metadata": {
        "id": "xhLthgCosJBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### evaluate using model A\n",
        "model_A.evaluate(np.expand_dims(ravdess_speech_data_array[training_samples + validation_samples:],-1), labels_categorical[training_samples + validation_sample])"
      ],
      "metadata": {
        "id": "UB6qa57ZtBBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions = {1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fearful', 7:'disgust', 8:'surprised'}\n",
        "def predict(wav_filepath):\n",
        "  test_point = extract_mfcc(wav_filepath)\n",
        "  test_point = np.reshape(test_point, neshape=(1,40,1))\n",
        "  predictions= model_A.predict(test_point)\n",
        "  print(emotions[np.argmax(predictions[0])+1])"
      ],
      "metadata": {
        "id": "NegjZqFhtBbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict('')"
      ],
      "metadata": {
        "id": "rdAKjhnuu_-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict('')"
      ],
      "metadata": {
        "id": "CJBYEge0vAH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict('')"
      ],
      "metadata": {
        "id": "JD-zx8aWvOK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict('')"
      ],
      "metadata": {
        "id": "6OQiGrRcvPoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict('')"
      ],
      "metadata": {
        "id": "3o62ahScvhea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_A.save('mymodel.h5')"
      ],
      "metadata": {
        "id": "VIOFY-2NvmBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelc = tf.keras,models.load_model('mymodel.h5')"
      ],
      "metadata": {
        "id": "MQZA3YSkwK7X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}